{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adb2fa09-761f-4796-b227-688ea6d3086c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Function to load validation data (for example purposes, using sklearn's load_iris dataset)\n",
    "from sklearn.datasets import load_iris\n",
    "def load_validation_data():\n",
    "    data = load_iris()\n",
    "    return data.data, data.target\n",
    "\n",
    "# Function to validate model\n",
    "def validate_model(model_uri):\n",
    "    # Load the model from the MLflow registry\n",
    "    model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "    # Load validation data\n",
    "    X_val, y_val = load_validation_data()\n",
    "\n",
    "    # Predict with the model\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    # Evaluate performance metrics\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred, average='weighted')\n",
    "    recall = recall_score(y_val, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_val, y_pred, average='weighted')   \n",
    "\n",
    "    # Log classification report (optional, as a summary string)\n",
    "    clf_report = classification_report(y_val, y_pred, output_dict=True)\n",
    "        \n",
    "    # Print out the evaluation metrics\n",
    "    print(f\"Model Accuracy: {accuracy}\")\n",
    "    print(f\"Model Precision: {precision}\")\n",
    "    print(f\"Model Recall: {recall}\")\n",
    "    print(f\"model f1: {f1}\")\n",
    "\n",
    "    # Optionally log metrics to MLflow (optional step)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "    mlflow.log_dict(clf_report, \"classification_report.json\")\n",
    "    # Check if model meets performance criteria (e.g., accuracy > 0.9)\n",
    "    if accuracy < 0.9:\n",
    "        print(\"Model failed validation criteria. Exiting.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_id='0cd5e4783cae4db7be2359bf75b65fbc'\n",
    "    model_uri=f'runs:/{run_id}/model'\n",
    "    validate_model(model_uri)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "model_validation_iris",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
