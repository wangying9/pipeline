{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b192a23-1517-4dca-8b89-a690ae33bf97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# Train a new model, log it to MLflow, and log evaluation metrics including precision, recall, F1 score, etc.\n",
    "def train_and_log_model():\n",
    "    # Load data\n",
    "    data = load_iris()\n",
    "    X, y = data.data, data.target\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Start an MLflow run to log the model and metrics\n",
    "    with mlflow.start_run():\n",
    "        # Train model\n",
    "        model = RandomForestClassifier(n_estimators=100)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Calculate accuracy score\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "        # Log precision score\n",
    "        precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "        mlflow.log_metric(\"precision\", precision)\n",
    "\n",
    "        # Log recall score\n",
    "        recall = recall_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "        mlflow.log_metric(\"recall\", recall)\n",
    "\n",
    "        # Log F1 score\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "        # Log classification report (optional, as a summary string)\n",
    "        clf_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        mlflow.log_dict(clf_report, \"classification_report.json\")\n",
    "\n",
    "        # Log the model with signature and input example\n",
    "        signature = infer_signature(X_test, y_pred)\n",
    "        mlflow.sklearn.log_model(model, \"model\", signature=signature, input_example=X_test[0])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_and_log_model()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "train_log_model_iris",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
